\section{کاربردها}

\subsection{بررسی اثر گذاری}

در قسمت قبلی، رویکرد های جدید، و به کل زاویه دید جدیدی بر روی مسائل پیشین داشتیم، که با استفاده از هندسه خاصه میتوانیم اقدام به بهینه کردن آن روش ها و به معرفی روش ها بپردازیم، برای مثال در \cite{EM-info} روش جدید بر روی الگگوریتم EM معرفی میکند، \cite{Morales_2021} به تعمیم دادن قاعده بیشینه آنتروپی میپردازد، و \cite{han2020from} به یادگیری VAE ها میپردازد.

به طور کلی، هندسه اطلاعات میتواند زاویه جدیدی به مسائل زیادی در علوم اطلاعات بیاورد، برای مثال:

\begin{itemize}
	\item \textbf{آمار:} الگوریتم هایی مانند EM، و مدل های ARMA
	\item \textbf{یادگیری ماشین:} مدل های بولتزمن، رویه های عصبی، گرادیان طبیعی(در ادامه بررسی میشود)
	\item \textbf{پردازش سیگنال:} آنالیز منابع مستقل(ICA)، تجزیه نامنفی ماتریس(NMF)
	\item \textbf{برنامه ریزی ریاضی:} روش های مبتنی بر بهینه سازی، برای مثال روش الگوریتم تکاملی طبیعی(NES)
	\item \textbf{تئوری بازی:} توابع امتیاز دهی(\lr{Score functions})
\end{itemize}

در ادامه به برخی از الگوریتم ها، بخصوص آنهایی که مستقیما از هندسه اطلاعات بر می آیند را بررسی میکنیم.

\subsection{گرادیان طبیعی(NGD)}

ابتدا به الگوریتم گرادیان نزولی معمولی نگاه میکنیم، و مشکلات آن را زیر نظر قرار میدهیم.
$$
	\text{GD:}\quad \theta_{t+1}=\theta_t-\alpha_t\nabla_\theta L_\theta(\theta_t)
$$
نکته این است که اگر حال ما توزیع خود، و در عمل تابع زیان خود را با پارامتر بندی دیگری مانند $\eta=\eta(\theta)$ پارامتر بندی کنیم، مسیر بهینه سازی به طور کلی متفاوت خواهد بود، زیرا گرادیان های متفاوتی داریم، و در حالت کلی ممکن است به مینیمم های محلی متفاوتی منجر شوند.
\begin{multline*}
	\eta_{t+1}=\eta_t-\alpha_t\nabla_\eta L_\eta(\eta_t)=\eta_t-\alpha_t\nabla_\eta L_\theta(\theta_t)
	=\eta_t-\alpha_t\left[\frac{\partial \theta}{\partial \eta}\right]\nabla_\theta L_\theta(\theta_t)\\=\eta(\theta_t)-\alpha_t\left[\frac{\partial \theta}{\partial \eta}\right]\nabla_\theta L_\theta(\theta_t)\approx\eta(\theta_t-\alpha_t\left[\frac{\partial \eta}{\partial \theta}\right]^{-1}\left[\frac{\partial \theta}{\partial \eta}\right]\nabla_\theta L_\theta(\theta_t))\neq \eta(\theta_{t+1})
\end{multline*}

این اتفاق با انتظار ما از ناوردایی نثبت به پارامتر بندی مطابق نیست، در نتیجه مطلوب نیست.
همچنین در نزدیکی نقطه بهینه، به علت محو شدن اطلاعات فیشر، نرخ بهینه شدن به شدت افت میکند.

با استفاده از هندسه، و روش هایی که آنجا استفاده میکردیم، میفهمیم که باید در واقع گرادیان را در جهت بیشینه کاهش ریمانی در نظر گرفت، نه اقلیدسی، که در نهایت به این معنی است که باید از متریک استفاده کنیم.

$$
	\text{NGD:}\quad \theta_{t+1}=\theta_t-\alpha_tg(\theta)^{-1}\nabla_\theta L_\theta(\theta_t)=\theta_t-\alpha_tI(\theta)^{-1}\nabla_\theta L_\theta(\theta_t)
$$

ابتدا، در عمل میبینیم که این مشکل محو شدن گرادیان در نزدیکی نقطه بهینه را رفع میکند، و همچنین ناوردایی را همراه خود میاورد.

\begin{multline*}
	\eta_{t+1}=\eta_t-\alpha_tI(\eta)^{-1}\nabla_\eta L_\eta(\eta_t)=\eta_t-\alpha_tI(\eta)^{-1}\nabla_\eta L_\theta(\theta_t)
	=\eta_t-\alpha_t\left[\frac{\partial \eta}{\partial \theta}\right]I(\theta)^{-1}\left[\frac{\partial \eta}{\partial \theta}\right]\left[\frac{\partial \theta}{\partial \eta}\right]\-\\\-\cdot\nabla_\theta L_\theta(\theta_t)=\eta(\theta_t)-\alpha_t\left[\frac{\partial \eta}{\partial \theta}\right]\nabla_\theta L_\theta(\theta_t)\approx\eta(\theta_t-\alpha_tI(\theta)^{-1}\nabla_\theta L_\theta(\theta_t))= \eta(\theta_{t+1})
\end{multline*}

در واقع دیگر بهینه سازی ما وابسته به پارامتر بندی و تعریف مسئله نیست، میتوانیم حتی بگوییم معادل با بهترین پارامتر بندی است!

\subsubsection{پیاده سازی}

در اینجا، برای دیدن کاربردی بودن این الگوریتم، یک مسئله ساده را در نظر میگیرم.

اینجا، تلاش میکنیم تا بین دو توزیع گوسی، درون یابی کنیم، و از دو نوع تابع زیان متفاوت، یکی انحراف کولبک لیبر، و دیگری فاصله واشراشتاین استفاهد میکنیم.
در پیاده سازی خود، باری در تعریف از گوسی با پارامتر بندی $(\mu,\sigma)$ و بار دیگر با $(\mu, \sigma^2)$ جلو میرویم، تا تفاوت ها و اثر ناوردایی را مشاهده کنیم.

\begin{figure}[h]
	\centering
	\includegraphics*[width=0.49\linewidth]{Pictures/Q6/NG-KL.jpg}
	\includegraphics*[width=0.49\linewidth]{Pictures/Q6/NG-Wasserstein.jpg}
	\caption{روند بهینه سازی، تابع زیان کولبک لیبر(راست)، واشراشتاین(چپ)}
\end{figure}

در پیاده سازی خود میبینیم که، مسیر های بسیار متفاوتی طی میکنند، ولی با استفاده از گرادیان طبیعی، جفت مسیر ها یکسان است. همچنین مشاهده میکنیم که نزدیک به نقطه بهینه، گرادیان تقریبا محو شده و ایتریشن های بسیار زیادی طول میکشد، در جدول زیر عملکرد را میبینیم.

\begin{table}[h]
	\centering
	\begin{latin}
		\begin{tabular}{lrrr}
			% \toprule
			\hline
			$L(\theta)$          & \textbf{NGD} & \textbf{GD: $(\mu, \sigma)$} & \textbf{GD: $(\mu, \sigma^2)$} \\
			% \midrule
			\hline
			\textbf{KL}          & 67           & 214                        & 1288                         \\
			\textbf{Wasserstein} & 164          & 223                        & 2595                         \\
			% \bottomrule
			\hline
		\end{tabular}
	\end{latin}
	\caption{مقایسه بین الگوریتم ها، تعداد مرحله طی شده برای بهینه سازی}
	\label{tab:distances}
\end{table}

که نتایج جدول \ref*{tab:distances} برای ما واضح میکند که الگوریتم گرادیان نزولی طبیعی روش خوبی است.

\subsection{تایین توزیع پیشین پیوسته}

یکی از کاربردهای دیگری که می‌توان به آن اشاره کرد برای زمانی است که می‌خواهیم توزیع پیشین روی پارامترها در یک مساله تخمین در نظر بگیریم. اگر اطلاعاتی در مورد توزیع پیشین نداشته باشیم علاقه داریم که توزیعی انتخاب کنیم که بیشترین ابهام را داشته باشد. اگر تخمین بین چند گزینه باشد یا به عبارتی پارامتر گسسته باشد به راحتی می‌توانیم توزیع یکنواخت را برای آن ها در نظر بگیریم اما برای وقتی که پارامترها پیوسته اند این سوال پیش می‌آید که توزیع یکنواخت نسبت به چه پارامتربندی ای؟ در اینجا میتوانیم به تعریف عنصر حجم ناورا نگاه کنیم:
\begin{equation}
    \omega=\sqrt{\det g}\ d\theta^1\wedge d\theta^2\wedge \dots \wedge d\theta^N
\end{equation}
اگر بخواهیم مشابه حالت گسسته عمل کنیم میتوانیم جواب را اینگونه در نظر بگیریم که توزیع باید متناسب باشد با $\sqrt{\det g}$ یعنی داریم:
\begin{equation}
    \pi(\theta)\propto \sqrt{\det g}
\end{equation}
در اینجا $\pi$ توزیع پیشین روی پارامتر است. که ضریب تناسب از بهنجار بودن توزیع احتمال بدست میاید.
بنابراین با دید هندسی توانستیم به این سوال نابدیهی که برای مورد پیوسته چه توزیعی را میتوانیم معادل توزیع یکنواخت در نظر گرفت پیدا کرد. همانطور که گفته شده نابدیهی بودن از اینجا میاید که یکنواخت بودن نسبت به یک پارامتر یکنواخت بودن نسبت به پارامتر دیگر را نتیجه نمی دهد. اما با استفاده از فرم حجم ناوردا این مشکل برطرف می‌شود.